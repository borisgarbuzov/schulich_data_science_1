{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tibshirani_Ch2_spam(SVC vs MultinomialNB).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"FaYIF3b4m-BS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621707811893,"user_tz":-180,"elapsed":34513,"user":{"displayName":"Boris Garbuzov","photoUrl":"","userId":"08851466340381164023"}},"outputId":"5fd57ee6-9de8-4518-face-845c4a4292df"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bb9QU68dSYEH","executionInfo":{"status":"ok","timestamp":1621707811898,"user_tz":-180,"elapsed":106,"user":{"displayName":"Boris Garbuzov","photoUrl":"","userId":"08851466340381164023"}}},"source":["# creating word dictionary\n","def make_Dictionary(train_dir):\n","    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n","    all_words = []       \n","    for mail in emails:\n","        filename, file_extension = os.path.splitext(mail)\n","        if file_extension != \".txt\":\n","            continue\n","        with open(mail) as m:\n","            for i,line in enumerate(m):\n","                if i == 2:  #Body of email is only 3rd line of text file\n","                    words = line.split()\n","                    all_words += words\n","\n","    dictionary = Counter(all_words)\n","    list_to_remove = [*dictionary]\n","    \n","    for item in list_to_remove:\n","        if item.isalpha() == False: \n","            del dictionary[item]\n","        elif len(item) == 1:\n","            del dictionary[item]\n","        # dictionary = dictionary.most_common(3000)\n","    return dictionary\n"," "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9KP676ZSYND","executionInfo":{"status":"ok","timestamp":1621707811904,"user_tz":-180,"elapsed":78,"user":{"displayName":"Boris Garbuzov","photoUrl":"","userId":"08851466340381164023"}}},"source":["#feature extraction process\n","def extract_features(mail_dir): \n","    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n","    print(len(files))\n","    features_matrix = np.zeros((len(files),17000))\n","    docID = 0;\n","    for fil in files:\n","      filename, file_extension = os.path.splitext(fil)\n","      if file_extension != \".txt\":\n","            continue\n","      with open(fil) as fi:\n","        for i,line in enumerate(fi):\n","          if i == 2:\n","            words = line.split()\n","            for word in words:\n","              wordID = 0\n","              for i,d in enumerate(dictionary):\n","                if d[0] == word:\n","                  wordID = i\n","                  # print(\"{} - {}\".format(docID, wordID))\n","                  # print(\"---------------\")\n","                  features_matrix[docID,wordID] = words.count(word)\n","        docID = docID + 1     \n","    return features_matrix"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEr4RjUOSYUo","executionInfo":{"status":"ok","timestamp":1621707811906,"user_tz":-180,"elapsed":77,"user":{"displayName":"Boris Garbuzov","photoUrl":"","userId":"08851466340381164023"}}},"source":[""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bZU7i5XSYaD","executionInfo":{"status":"ok","timestamp":1621707811908,"user_tz":-180,"elapsed":78,"user":{"displayName":"Boris Garbuzov","photoUrl":"","userId":"08851466340381164023"}}},"source":[""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCNuphYxd_u8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c194bae4-f754-4a15-baa8-4bfb7102c2ce"},"source":["# training the classifiers\n","import os\n","import numpy as np\n","from collections import Counter\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n","from sklearn.svm import SVC, NuSVC, LinearSVC\n"," \n","# Create a dictionary of words with its frequency\n","\n"," \n","train_dir = '/content/drive/My Drive/Colab Notebooks/York/ds1/data/train-mails/'\n","dictionary = make_Dictionary(train_dir)\n"," \n","# Prepare feature vectors per training mail and its labels\n"," \n","train_labels = np.zeros(702)\n","train_labels[351:701] = 1\n","train_matrix = extract_features(train_dir)\n"," \n","# Training SVM and Naive bayes classifier\n"," \n","model1 = MultinomialNB()\n","model2 = LinearSVC()\n","model1.fit(train_matrix,train_labels)\n","model2.fit(train_matrix,train_labels)\n"," \n","# Test the unseen mails for Spam\n","test_dir = '/content/drive/My Drive/Colab Notebooks/York/ds1/data/test-mails/'\n","test_matrix = extract_features(test_dir)\n","test_labels = np.zeros(260)\n","test_labels[130:260] = 1\n","result1 = model1.predict(test_matrix)\n","result2 = model2.predict(test_matrix)\n","print(confusion_matrix(test_labels,result1))\n","print(confusion_matrix(test_labels,result2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["703\n"],"name":"stdout"}]}]}